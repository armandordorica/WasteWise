# -*- coding: utf-8 -*-
"""FineTunedBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SvLt4r_Jm4PrSzN5r2yogqAPgS1iACCy
"""

import torch
from torch.nn.functional import softmax
from transformers import BertTokenizer, BertForSequenceClassification, AutoConfig
import pickle
import requests

class BERTClassifier:
    def __init__(self, model_identifier):
        # Load the tokenizer from bert base uncased
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        # Load the config
        config = AutoConfig.from_pretrained(model_identifier)

        # Load the model
        self.model = BertForSequenceClassification.from_pretrained(model_identifier, config=config)
        self.model.eval()  # Set the model to evaluation mode

        # Load the label encoder
        encoder_url = f'https://huggingface.co/{model_identifier}/resolve/main/model_encoder.pkl'
        self.labels = pickle.loads(requests.get(encoder_url).content)

    def predict_category(self, text):
        # Tokenize the text
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)

        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # Calculate the confidence using softmax
        probs = softmax(outputs.logits, dim=1)

        # Get the prediction index
        prediction_idx = torch.argmax(outputs.logits, dim=1).item()
        confidence = probs[0, prediction_idx].item()

        # Decode the prediction index to get the label
        prediction_label = self.labels[prediction_idx]  # Use indexing for a NumPy array

        return prediction_label, confidence
